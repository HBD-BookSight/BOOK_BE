package com.hbd.book_be.external.loader

import com.fasterxml.jackson.dataformat.csv.CsvMapper
import com.fasterxml.jackson.dataformat.csv.CsvSchema
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule
import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper
import com.hbd.book_be.config.properties.ExternalLoaderProperties
import com.hbd.book_be.dto.request.BookCreateRequest
import com.hbd.book_be.exception.ErrorCodes
import com.hbd.book_be.exception.ValidationException
import com.hbd.book_be.external.kakao.KakaoApiRequest
import com.hbd.book_be.external.kakao.KakaoBookSearchClient
import com.hbd.book_be.external.loader.dto.CulturalBookDto
import com.hbd.book_be.util.DateUtil
import org.slf4j.LoggerFactory
import org.springframework.boot.CommandLineRunner
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty
import org.springframework.jdbc.core.JdbcTemplate
import org.springframework.stereotype.Component
import java.io.BufferedWriter
import java.io.FileWriter
import java.nio.file.Files
import java.nio.file.Paths

@Component
@ConditionalOnProperty(
    prefix = "external.cultural-data-loader",
    name = ["enabled"],
    havingValue = "true",
    matchIfMissing = false
)
class CulturalDatasetLoader(
    jdbcTemplate: JdbcTemplate,
    private val kakaoBookSearchClient: KakaoBookSearchClient,
    private val loaderProperties: ExternalLoaderProperties
) : CommandLineRunner {

    private val log = LoggerFactory.getLogger(CulturalDatasetLoader::class.java)

    private val jdbcRepository = BookJdbcRepository(jdbcTemplate)
    private val mapper = jacksonObjectMapper().apply {
        registerModule(JavaTimeModule())
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
    }

    private val outputPath = Paths.get(loaderProperties.outputPath)
    private val progressPath = Paths.get(loaderProperties.progressPath ?: "${loaderProperties.outputPath}.progress")
    private val csvChunkSize = 1000

    override fun run(vararg args: String?) {
        log.info("[ğŸš€] CulturalDatasetLoader ì‹œì‘ë¨ (external-loader.enabled=true)")

        initializeOutputFiles()
        checkExistingFiles()

        val csvFile = "dataset.csv"
        log.info("[ğŸ“‚] ì²˜ë¦¬ ì¤‘: $csvFile")

        // 1ë‹¨ê³„: JSONL ìƒì„±
        val allEnrichedRequests = processCsvFileFromProgress(csvFile)

        // 2ë‹¨ê³„: DB ì €ì¥
        log.info("[ğŸ’¾] JSONL ìƒì„± ì™„ë£Œ. ì´ì œ DB ì €ì¥ ì‹œì‘...")
        saveToDatabase(allEnrichedRequests)

        // ì™„ë£Œ í›„ ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
        clearProgress()
        log.info("[ğŸ‰] ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
    }

    private fun checkExistingFiles() {
        // ê¸°ì¡´ JSONL íŒŒì¼ í™•ì¸
        if (Files.exists(outputPath)) {
            val existingLines = Files.lines(outputPath).use { it.count() }
            log.info("[ğŸ“‹] ê¸°ì¡´ JSONL íŒŒì¼ ë°œê²¬: ${existingLines}ì¤„")
        } else {
            log.info("[ğŸ“‹] ìƒˆë¡œìš´ JSONL íŒŒì¼ ìƒì„± ì˜ˆì •")
        }

        // ê¸°ì¡´ progress.txt í™•ì¸
        if (Files.exists(progressPath)) {
            val progressInfo = Files.readString(progressPath).trim()
            log.info("[ğŸ“‹] ê¸°ì¡´ ì§„í–‰ ìƒí™© ë°œê²¬: $progressInfo")
        } else {
            log.info("[ğŸ“‹] ì²˜ìŒë¶€í„° ì‹œì‘")
        }
    }

    private fun initializeOutputFiles() {
        Files.createDirectories(outputPath.parent)
        Files.createDirectories(progressPath.parent)
    }

    private fun processCsvFileFromProgress(csvFileName: String): List<BookCreateRequest> {
        val startChunkIndex = loadProgress(csvFileName)
        val totalRows = getTotalCsvRows(csvFileName)
        val totalChunks = (totalRows + csvChunkSize - 1) / csvChunkSize
        val skipRows = startChunkIndex * csvChunkSize

        log.info("[ğŸ“Š] $csvFileName: ì´ ${totalRows}í–‰, ${totalChunks}ì²­í¬ ì¤‘ ${startChunkIndex}ë²ˆì§¸ë¶€í„° ì‹œì‘")
        log.info("[â­ï¸] ${skipRows}í–‰ ìŠ¤í‚µí•˜ê³  ${skipRows + 1}í–‰ë¶€í„° ì²˜ë¦¬ ì‹œì‘")

        val csvMapper = CsvMapper()
        val schema = CsvSchema.emptySchema().withHeader()
        val allEnrichedRequests = mutableListOf<BookCreateRequest>()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val reader = csvMapper.readerFor(CulturalBookDto::class.java).with(schema)
            val iterator = reader.readValues<CulturalBookDto>(inputStream)

            // ì´ë¯¸ ì²˜ë¦¬ëœ í–‰ë“¤ ìŠ¤í‚µ
            log.info("[â©] ${skipRows}í–‰ ìŠ¤í‚µ ì¤‘...")
            var skippedCount = 0
            repeat(skipRows) {
                if (iterator.hasNext()) {
                    iterator.next()
                    skippedCount++
                    if (skippedCount % 10000 == 0) {
                        log.info("[â©] ${skippedCount}/${skipRows}í–‰ ìŠ¤í‚µ ì™„ë£Œ...")
                    }
                }
            }
            log.info("[âœ…] ìŠ¤í‚µ ì™„ë£Œ. ì´ì œ ${skipRows + 1}í–‰ë¶€í„° ì²˜ë¦¬ ì‹œì‘")

            var currentChunkIndex = startChunkIndex
            var chunk = mutableListOf<CulturalBookDto>()

            while (iterator.hasNext()) {
                val dto = iterator.next()
                chunk.add(dto)

                if (chunk.size >= csvChunkSize) {
                    val enrichedRequests = processChunk(chunk, currentChunkIndex)
                    allEnrichedRequests.addAll(enrichedRequests)
                    chunk.clear()
                    currentChunkIndex++

                    // ì§„í–‰ ìƒí™© ì €ì¥
                    saveProgress(csvFileName, currentChunkIndex)

                    val processedRows = currentChunkIndex * csvChunkSize
                    val progress = (processedRows.toDouble() / totalRows * 100).toInt()
                    log.info("[ğŸ“Š] $csvFileName: ${processedRows}/${totalRows}í–‰ ì²˜ë¦¬ ì™„ë£Œ (${progress}%) - ì²­í¬ #${currentChunkIndex}")
                }
            }

            // ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if (chunk.isNotEmpty()) {
                val enrichedRequests = processChunk(chunk, currentChunkIndex)
                allEnrichedRequests.addAll(enrichedRequests)
                currentChunkIndex++
                saveProgress(csvFileName, currentChunkIndex)
                log.info("[âœ…] $csvFileName: JSONL ìƒì„± ì™„ë£Œ")
            }
        } ?: throw ValidationException(
            "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName",
            ErrorCodes.CSV_FILE_NOT_FOUND
        )

        return allEnrichedRequests
    }

    private fun getTotalCsvRows(csvFileName: String): Int {
        val csvMapper = CsvMapper()
        val schema = CsvSchema.emptySchema().withHeader()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val reader = csvMapper.readerFor(CulturalBookDto::class.java).with(schema)
            val iterator = reader.readValues<CulturalBookDto>(inputStream)

            var count = 0
            while (iterator.hasNext()) {
                iterator.next()
                count++
            }
            return count
        } ?: throw ValidationException(
            "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName",
            ErrorCodes.CSV_FILE_NOT_FOUND
        )
    }

    private fun processChunk(chunk: List<CulturalBookDto>, chunkIndex: Int): List<BookCreateRequest> {
        val requests = parseToRequests(chunk)
        val enrichedRequests = mutableListOf<BookCreateRequest>()

        requests.forEach { request ->
            val enrichedRequest = enrichBookRequest(request)
            enrichedRequests.add(enrichedRequest)
        }

        // JSONL í˜•íƒœë¡œ ì €ì¥ (DB ì €ì¥ì€ ë‚˜ì¤‘ì—)
        appendRequestsToJsonl(enrichedRequests)

        return enrichedRequests
    }

    private fun appendRequestsToJsonl(requests: List<BookCreateRequest>) {
        BufferedWriter(FileWriter(outputPath.toFile(), true)).use { writer ->
            requests.forEach { request ->
                val jsonString = mapper.writeValueAsString(request)
                writer.write(jsonString)
                writer.write("\n")
            }
        }
    }

    // JSONL ìƒì„± ì™„ë£Œ í›„ DBì— ì €ì¥
    private fun saveToDatabase(allEnrichedRequests: List<BookCreateRequest>) {
        log.info("[ğŸ’¾] ì´ ${allEnrichedRequests.size}ê°œ ë°ì´í„°ë¥¼ DBì— ì €ì¥ ì‹œì‘...")

        allEnrichedRequests.chunked(loaderProperties.batchSize).forEachIndexed { idx, chunk ->
            try {
                jdbcRepository.saveBooksWithJdbc(chunk)
                log.info("[âœ…] ${idx + 1}ë²ˆì§¸ DB ì²­í¬ ì €ì¥ ì„±ê³µ (${chunk.size}ê¶Œ)")
            } catch (e: Exception) {
                log.error("[âŒ] ${idx + 1}ë²ˆì§¸ DB ì²­í¬ ì €ì¥ ì‹¤íŒ¨: ${e.message}", e)
                throw e // ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
            }
        }

        log.info("[ğŸ‰] DB ì €ì¥ ì™„ë£Œ: ${allEnrichedRequests.size}ê°œ ë°ì´í„°")
    }

    // ì§„í–‰ ìƒí™© ì €ì¥: "íŒŒì¼ëª…:ì²­í¬ì¸ë±ìŠ¤" í˜•íƒœ
    private fun saveProgress(csvFileName: String, chunkIndex: Int) {
        val progressInfo = "$csvFileName:$chunkIndex"
        Files.writeString(progressPath, progressInfo)
    }

    // ì§„í–‰ ìƒí™© ë¡œë“œ
    private fun loadProgress(csvFileName: String): Int {
        return if (Files.exists(progressPath)) {
            val progressInfo = Files.readString(progressPath).trim()
            val parts = progressInfo.split(":")
            if (parts.size == 2 && parts[0] == csvFileName) {
                parts[1].toIntOrNull() ?: 0
            } else {
                0 // ë‹¤ë¥¸ íŒŒì¼ì´ê±°ë‚˜ í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ ì²˜ìŒë¶€í„°
            }
        } else {
            0
        }
    }

    // ì²˜ë¦¬ ì™„ë£Œ í›„ ì§„í–‰ ìƒí™© íŒŒì¼ ì‚­ì œ
    private fun clearProgress() {
        if (Files.exists(progressPath)) {
            Files.delete(progressPath)
        }
    }

    private fun enrichBookRequest(request: BookCreateRequest): BookCreateRequest {
        val isIsbnSearchable = request.isbn != "UNKNOWN" && request.isbn.isNotBlank()

        val kakaoRequest = KakaoApiRequest(
            query = if (isIsbnSearchable) request.isbn else request.title,
            target = if (isIsbnSearchable) "isbn" else "title",
            size = 1
        )

        val kakaoResponse = kakaoBookSearchClient.searchBook(kakaoRequest)
        val doc = kakaoResponse?.documents?.firstOrNull()
        val publishedDate = doc?.datetime
            ?.takeIf { it.length >= 10 }
            ?.substring(0, 10)
            ?.let { DateUtil.parseFlexibleDate(it) }
            ?: DateUtil.parseFlexibleDate("0001-01-01")

        return if (doc != null) {
            request.copy(
                summary = doc.contents,
                publishedDate = publishedDate,
                detailUrl = doc.url,
                translator = doc.translators,
                price = doc.price,
                titleImage = doc.thumbnail,
                authorNameList = doc.authors,
                publisherName = doc.publisher
            )
        } else {
            log.info("[âš ï¸] '${request.title} ${request.isbn}' enrich ì‹¤íŒ¨ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
            request
        }
    }

    private fun parseToRequests(dataList: List<CulturalBookDto>): List<BookCreateRequest> {
        return dataList.mapNotNull { dto ->
            try {
                val (authors, translators) = parseContributors(dto.authrNm)
                BookCreateRequest(
                    isbn = dto.isbnThirteenNo ?: dto.isbnNo ?: "UNKNOWN",
                    title = dto.titleNm ?: "ì œëª© ì—†ìŒ",
                    summary = dto.bookIntrcnCn.orEmpty(),
                    publishedDate = DateUtil.parseFlexibleDate(
                        (dto.pblicteDe ?: dto.twoPblicteDe).takeIf { !it.isNullOrBlank() } ?: "1001-01-01"
                    ),
                    detailUrl = null,
                    translator = translators,
                    price = dto.prcValue?.toIntOrNull(),
                    titleImage = dto.imageUrl,
                    authorNameList = authors,
                    publisherName = dto.publisherNm ?: "ì•Œ ìˆ˜ ì—†ìŒ"
                )
            } catch (e: Exception) {
                log.info("[âš ï¸] íŒŒì‹± ì‹¤íŒ¨: ${dto.titleNm} (${e.message})")
                null
            }
        }
    }

    private fun parseContributors(raw: String?): Pair<List<String>, List<String>> {
        val authors = mutableListOf<String>()
        val translators = mutableListOf<String>()
        raw?.split(",")?.map { it.trim() }?.forEach { person ->
            when {
                person.contains("ì§€ì€ì´") -> authors.add(person.replace("(ì§€ì€ì´)", "").trim())
                person.contains("ì˜®ê¸´ì´") -> translators.add(person.replace("(ì˜®ê¸´ì´)", "").trim())
            }
        }
        return authors to translators
    }
}