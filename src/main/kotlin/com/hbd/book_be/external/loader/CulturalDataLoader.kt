package com.hbd.book_be.external.loader

import com.fasterxml.jackson.dataformat.csv.CsvMapper
import com.fasterxml.jackson.dataformat.csv.CsvSchema
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule
import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper
import com.hbd.book_be.config.properties.ExternalLoaderProperties
import com.hbd.book_be.dto.request.BookCreateRequest
import com.hbd.book_be.exception.ErrorCodes
import com.hbd.book_be.exception.ValidationException
import com.hbd.book_be.external.kakao.KakaoApiRequest
import com.hbd.book_be.external.kakao.KakaoBookSearchClient
import com.hbd.book_be.external.loader.dto.CulturalBookDto
import com.hbd.book_be.util.DateUtil
import org.slf4j.LoggerFactory
import org.springframework.boot.CommandLineRunner
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty
import org.springframework.jdbc.core.JdbcTemplate
import org.springframework.stereotype.Component
import java.io.BufferedWriter
import java.io.FileWriter
import java.nio.file.Files
import java.nio.file.Paths

@Component
@ConditionalOnProperty(
    prefix = "external.cultural-data-loader",
    name = ["enabled"],
    havingValue = "true",
    matchIfMissing = false
)
class CulturalDatasetLoader(
    jdbcTemplate: JdbcTemplate,
    private val kakaoBookSearchClient: KakaoBookSearchClient,
    private val loaderProperties: ExternalLoaderProperties
) : CommandLineRunner {

    private val log = LoggerFactory.getLogger(CulturalDatasetLoader::class.java)

    private val jdbcRepository = BookJdbcRepository(jdbcTemplate)
    private val mapper = jacksonObjectMapper().apply {
        registerModule(JavaTimeModule())
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
        disable(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)
    }

    private val outputPath = Paths.get(loaderProperties.outputPath)
    private val progressPath = Paths.get(loaderProperties.progressPath ?: "${loaderProperties.outputPath}.progress")
    private val fileSize = 5000 // íŒŒì¼ë‹¹ ë¼ì¸ ìˆ˜ (CSV ì˜ˆìƒ + JSONL ì €ì¥ ë‹¨ìœ„)
    private val totalFileCount = 32 // dataset-1.csv ~ dataset-32.csv

    // JSONL ì²­í¬ ê´€ë¦¬ ë³€ìˆ˜
    private var currentJsonlChunkIndex = 0

    override fun run(vararg args: String?) {
        log.info("[ğŸš€] CulturalDatasetLoader ì‹œì‘ë¨ (external-loader.enabled=true)")

        initializeOutputFiles()
        checkExistingFiles()

        // ì§„í–‰ ìƒí™©ì—ì„œ ì‹œì‘í•  íŒŒì¼ ì¸ë±ìŠ¤ ë¡œë“œ
        val (startFileIndex, startLineNumber) = loadOverallProgress()

        // JSONL ì²­í¬ ìƒíƒœ ì´ˆê¸°í™” (ê¸°ì¡´ JSONL íŒŒì¼ ë¶„ì„)
        initializeJsonlChunk()

        // ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ì²´í¬
        if (startFileIndex > totalFileCount) {
            log.info("[ğŸ‰] ëª¨ë“  CSV íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. DB ì €ì¥ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
        } else {
            // CSV â†’ JSONL ì²˜ë¦¬
            processAllCsvFiles(startFileIndex, startLineNumber)
        }

        // JSONL â†’ DB ì €ì¥
        log.info("[ğŸ’¾] CSV ì²˜ë¦¬ ì™„ë£Œ. ì´ì œ JSONLì—ì„œ DB ì €ì¥ ì‹œì‘...")
        if (Files.exists(outputPath)) {
            saveJsonlToDatabase()
        } else {
            log.warn("[âš ï¸] JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        }

        log.info("[ğŸ‰] ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
    }

    private fun processAllCsvFiles(startFileIndex: Int, startLineNumber: Int) {
        // dataset-1.csvë¶€í„° dataset-32.csvê¹Œì§€ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for (fileIndex in startFileIndex..totalFileCount) {
            val csvFileName = "dataset-${fileIndex}.csv"
            log.info("[ğŸ“‚] ì²˜ë¦¬ ì¤‘: $csvFileName (${fileIndex}/${totalFileCount})")

            try {
                // ì²« ë²ˆì§¸ ì¬ì‹œì‘ íŒŒì¼ì¸ ê²½ìš° ì €ì¥ëœ ë¼ì¸ ë²ˆí˜¸ë¶€í„° ì‹œì‘, ê·¸ ì™¸ì—ëŠ” 0ë¶€í„° ì‹œì‘
                val startLineIndex = if (fileIndex == startFileIndex) startLineNumber else 0
                processCsvFileFromProgress(csvFileName, startLineIndex)

                log.info("[âœ…] $csvFileName ì²˜ë¦¬ ì™„ë£Œ")

                // íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ì‹œ ë‹¤ìŒ íŒŒì¼ë¡œ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                saveOverallProgress(fileIndex + 1, 0)

            } catch (e: Exception) {
                log.error("[âŒ] $csvFileName ì²˜ë¦¬ ì‹¤íŒ¨: ${e.message}", e)
                throw e
            }
        }
    }

    private fun checkExistingFiles() {
        // ê¸°ì¡´ JSONL íŒŒì¼ë“¤ í™•ì¸
        if (Files.exists(outputPath)) {
            Files.list(outputPath).use { files ->
                val jsonlFiles = files.filter { it.toString().endsWith(".jsonl") }.count()
                log.info("[ğŸ“‹] ê¸°ì¡´ JSONL íŒŒì¼ ${jsonlFiles}ê°œ ë°œê²¬")
            }
        } else {
            log.info("[ğŸ“‹] ìƒˆë¡œìš´ JSONL ë””ë ‰í† ë¦¬ ìƒì„± ì˜ˆì •")
        }

        // ê¸°ì¡´ progress.txt í™•ì¸
        if (Files.exists(progressPath)) {
            val progressInfo = Files.readString(progressPath).trim()
            log.info("[ğŸ“‹] ê¸°ì¡´ ì§„í–‰ ìƒí™© ë°œê²¬: $progressInfo")
        } else {
            log.info("[ğŸ“‹] ì²˜ìŒë¶€í„° ì‹œì‘")
        }
    }

    private fun initializeOutputFiles() {
        Files.createDirectories(outputPath.parent)
        Files.createDirectories(outputPath)
        Files.createDirectories(progressPath.parent)
    }

    private fun initializeJsonlChunk() {
        // ê¸°ì¡´ JSONL íŒŒì¼ë“¤ì„ ë¶„ì„í•´ì„œ í˜„ì¬ ì²­í¬ ì¸ë±ìŠ¤ ê²°ì •
        if (!Files.exists(outputPath)) {
            currentJsonlChunkIndex = 0
            return
        }

        Files.list(outputPath).use { files ->
            val jsonlFiles = files.filter { it.toString().endsWith(".jsonl") }.sorted().toList()
            if (jsonlFiles.isEmpty()) {
                currentJsonlChunkIndex = 0
                return
            }

            val lastFile = jsonlFiles.last()
            val lineCount = Files.lines(lastFile).use { it.count().toInt() }
            val lastChunkIndex = jsonlFiles.size - 1

            if (lineCount >= fileSize) {
                // í˜„ì¬ ì²­í¬ê°€ ê°€ë“ ì°¼ìœ¼ë©´ ë‹¤ìŒ ì²­í¬ ì‹œì‘
                currentJsonlChunkIndex = lastChunkIndex + 1
                log.info("[ğŸ“–] ë§ˆì§€ë§‰ ì²­í¬ê°€ ê°€ë“ í•¨, ìƒˆ ì²­í¬ #${currentJsonlChunkIndex} ì‹œì‘")
            } else {
                // í˜„ì¬ ì²­í¬ì—ì„œ ì´ì–´ì“°ê¸°
                currentJsonlChunkIndex = lastChunkIndex
                log.info("[ğŸ“–] ì²­í¬ #${currentJsonlChunkIndex}ì—ì„œ ì´ì–´ì“°ê¸° (í˜„ì¬ ${lineCount}ì¤„)")
            }
        }
    }

    private fun processCsvFileFromProgress(csvFileName: String, startLineIndex: Int = 0) {
        val totalRows = getTotalCsvRows(csvFileName)

        log.info("[ğŸ“Š] $csvFileName: ì´ ${totalRows}í–‰, ${startLineIndex}í–‰ë¶€í„° ì‹œì‘")
        if (startLineIndex > 0) {
            log.info("[â­ï¸] ${startLineIndex}í–‰ ìŠ¤í‚µí•˜ê³  ${startLineIndex + 1}í–‰ë¶€í„° ì²˜ë¦¬ ì‹œì‘")
        } else {
            log.info("[ğŸ¯] íŒŒì¼ ì‹œì‘ë¶€í„° ì²˜ë¦¬")
        }

        val csvMapper = createCsvMapper()
        val schema = CsvSchema.emptySchema().withHeader()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val reader = csvMapper.readerFor(CulturalBookDto::class.java).with(schema)
            val iterator = reader.readValues<CulturalBookDto>(inputStream)

            // ì´ë¯¸ ì²˜ë¦¬ëœ í–‰ë“¤ ìŠ¤í‚µ
            skipCsvRows(iterator, startLineIndex)

            var currentLineIndex = startLineIndex
            val currentFileIndex = csvFileName.removePrefix("dataset-").removeSuffix(".csv").toInt()

            while (iterator.hasNext()) {
                try {
                    val dto = iterator.next()
                    currentLineIndex++

                    // í•œ ì¤„ì”© ì¦‰ì‹œ ì²˜ë¦¬ â†’ JSONL ì €ì¥
                    val request = parseToRequest(dto)
                    if (request != null) {
                        val enrichedRequest = enrichBookRequest(request)

                        // ì¦‰ì‹œ JSONLì— ì €ì¥ (ë©”ëª¨ë¦¬ì— ë³´ê´€í•˜ì§€ ì•ŠìŒ)
                        appendSingleLineToJsonl(enrichedRequest)
                    }

                    // 100ì¤„ë§ˆë‹¤ Progress ì—…ë°ì´íŠ¸ ë° ë¡œê·¸
                    if (currentLineIndex % 100 == 0) {
                        saveOverallProgress(currentFileIndex, currentLineIndex)
                        val progress = (currentLineIndex.toDouble() / totalRows * 100).toInt()
                        log.info("[ğŸ“Š] $csvFileName: ${currentLineIndex}/${totalRows}í–‰ ì²˜ë¦¬ ì™„ë£Œ (${progress}%)")
                    }

                } catch (e: Exception) {
                    log.warn("[âš ï¸] CSV í–‰ ${currentLineIndex + 1} íŒŒì‹± ì˜¤ë¥˜ (ìŠ¤í‚µ): ${e.message}")
                    currentLineIndex++
                    continue
                }
            }

            // ìµœì¢… Progress ì—…ë°ì´íŠ¸
            saveOverallProgress(currentFileIndex, currentLineIndex)
            log.info("[âœ…] $csvFileName: ì´ ${currentLineIndex}í–‰ ì²˜ë¦¬ ì™„ë£Œ")

        } ?: throw ValidationException(
            "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName",
            ErrorCodes.CSV_FILE_NOT_FOUND
        )
    }

    private fun getTotalCsvRows(csvFileName: String): Int {
        val csvMapper = createCsvMapper()
        val schema = CsvSchema.emptySchema().withHeader()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val reader = csvMapper.readerFor(CulturalBookDto::class.java).with(schema)
            val iterator = reader.readValues<CulturalBookDto>(inputStream)

            var count = 0
            while (iterator.hasNext()) {
                try {
                    iterator.next()
                    count++
                } catch (e: Exception) {
                    log.warn("[âš ï¸] $csvFileName í–‰ ${count + 1} íŒŒì‹± ì˜¤ë¥˜ (ì¹´ìš´íŠ¸ë§Œ ì¦ê°€): ${e.message}")
                    count++
                }
            }
            log.info("[ğŸ“Š] $csvFileName ì´ í–‰ ìˆ˜: $count")
            return count
        } ?: throw ValidationException(
            "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName",
            ErrorCodes.CSV_FILE_NOT_FOUND
        )
    }

    private fun parseToRequest(dto: CulturalBookDto): BookCreateRequest? {
        return try {
            val (authors, translators) = parseContributors(dto.authrNm)
            BookCreateRequest(
                isbn = dto.isbnThirteenNo ?: dto.isbnNo ?: "UNKNOWN",
                title = dto.titleNm ?: "ì œëª© ì—†ìŒ",
                summary = dto.bookIntrcnCn.orEmpty(),
                publishedDate = DateUtil.parseFlexibleDate(
                    (dto.pblicteDe ?: dto.twoPblicteDe).takeIf { !it.isNullOrBlank() } ?: "1001-01-01"
                ),
                detailUrl = null,
                translator = translators,
                price = dto.prcValue?.toIntOrNull(),
                titleImage = dto.imageUrl,
                authorNameList = authors,
                publisherName = dto.publisherNm ?: "ì•Œ ìˆ˜ ì—†ìŒ"
            )
        } catch (e: Exception) {
            log.debug("[âš ï¸] íŒŒì‹± ì‹¤íŒ¨: ${dto.titleNm} (${e.message})")
            null
        }
    }

    private fun appendSingleLineToJsonl(request: BookCreateRequest) {
        val currentJsonlFile = outputPath.resolve("chunk-${String.format("%03d", currentJsonlChunkIndex)}.jsonl")

        // í˜„ì¬ íŒŒì¼ì˜ ë¼ì¸ ìˆ˜ í™•ì¸
        val currentLines = if (Files.exists(currentJsonlFile)) {
            Files.lines(currentJsonlFile).use { it.count().toInt() }
        } else {
            0
        }

        // 5000ì¤„ì´ ë˜ë©´ ë‹¤ìŒ íŒŒì¼ë¡œ
        if (currentLines >= fileSize) {
            currentJsonlChunkIndex++
            log.info("[ğŸ“„] ìƒˆë¡œìš´ JSONL ì²­í¬ ì‹œì‘: chunk-${String.format("%03d", currentJsonlChunkIndex)}.jsonl")
        }

        // í•œ ì¤„ ì¦‰ì‹œ ì €ì¥
        val targetJsonlFile = outputPath.resolve("chunk-${String.format("%03d", currentJsonlChunkIndex)}.jsonl")
        BufferedWriter(FileWriter(targetJsonlFile.toFile(), true)).use { writer ->
            writer.write(mapper.writeValueAsString(request))
            writer.write("\n")
        }
    }

    // JSONL íŒŒì¼ë“¤ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ì–´ì„œ ì²­í¬ ë‹¨ìœ„ë¡œ DB ì €ì¥
    private fun saveJsonlToDatabase() {
        Files.list(outputPath).use { files ->
            val jsonlFiles = files.filter { it.toString().endsWith(".jsonl") }.sorted().toList()

            log.info("[ğŸ“–] ì´ ${jsonlFiles.size}ê°œ JSONL íŒŒì¼ì—ì„œ ìŠ¤íŠ¸ë¦¬ë° DB ì €ì¥ ì‹œì‘")

            var totalSavedCount = 0
            var batch = mutableListOf<BookCreateRequest>()
            var batchIndex = 1

            jsonlFiles.forEach { jsonlFile ->
                log.info("[ğŸ“„] ${jsonlFile.fileName} ì²˜ë¦¬ ì¤‘...")

                Files.lines(jsonlFile).use { lines ->
                    lines.forEach { line ->
                        if (line.isNotBlank()) {
                            try {
                                val request = mapper.readValue(line, BookCreateRequest::class.java)
                                batch.add(request)

                                // ì²­í¬ í¬ê¸°ë§Œí¼ ëª¨ì´ë©´ DB ì €ì¥
                                if (batch.size >= loaderProperties.batchSize) {
                                    saveChunkToDatabase(batch, batchIndex)
                                    totalSavedCount += batch.size
                                    batch.clear()
                                    batchIndex++
                                }

                            } catch (e: Exception) {
                                log.warn("[âš ï¸] JSONL ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: $line")
                            }
                        }
                    }
                }

                log.info("[âœ…] ${jsonlFile.fileName} ì²˜ë¦¬ ì™„ë£Œ")
            }

            // ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ì €ì¥
            if (batch.isNotEmpty()) {
                saveChunkToDatabase(batch, batchIndex)
                totalSavedCount += batch.size
            }

            log.info("[ğŸ‰] ìŠ¤íŠ¸ë¦¬ë° DB ì €ì¥ ì™„ë£Œ: ì´ ${totalSavedCount}ê°œ ë°ì´í„°")
        }
    }

    // ì²­í¬ ë‹¨ìœ„ë¡œ DB ì €ì¥
    private fun saveChunkToDatabase(batch: List<BookCreateRequest>, batchIndex: Int) {
        try {
            jdbcRepository.saveBooksWithJdbc(batch)
            log.info("[âœ…] ${batchIndex}ë²ˆì§¸ DB ì²­í¬ ì €ì¥ ì„±ê³µ (${batch.size}ê¶Œ)")
        } catch (e: Exception) {
            log.error("[âŒ] ${batchIndex}ë²ˆì§¸ DB ì²­í¬ ì €ì¥ ì‹¤íŒ¨: ${e.message}", e)
            throw e // ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
        }
    }

    // ì „ì²´ ì§„í–‰ ìƒí™© ì €ì¥: "íŒŒì¼ì¸ë±ìŠ¤:ë¼ì¸ë²ˆí˜¸" í˜•íƒœ
    private fun saveOverallProgress(fileIndex: Int, lineNumber: Int) {
        val progressInfo = "$fileIndex:$lineNumber"
        Files.writeString(progressPath, progressInfo)
    }

    // ì „ì²´ ì§„í–‰ ìƒí™© ë¡œë“œ: (íŒŒì¼ì¸ë±ìŠ¤, ë¼ì¸ë²ˆí˜¸) ë°˜í™˜
    private fun loadOverallProgress(): Pair<Int, Int> {
        return if (Files.exists(progressPath)) {
            val progressInfo = Files.readString(progressPath).trim()
            log.info("[ğŸ“‹] Progress íŒŒì¼ ë°œê²¬: '$progressInfo'")

            val parts = progressInfo.split(":")
            when (parts.size) {
                2 -> {
                    // ê¸°ì¡´ ë˜ëŠ” ìƒˆ í˜•ì‹: íŒŒì¼ì¸ë±ìŠ¤:ë¼ì¸ë²ˆí˜¸
                    val fileIndex = parts[0].toIntOrNull() ?: 1
                    val lineNumber = parts[1].toIntOrNull() ?: 0

                    log.info("[ğŸ“‹] Progress: dataset-${fileIndex}.csvì˜ ${lineNumber}í–‰ë¶€í„° ì‹œì‘")
                    fileIndex to lineNumber
                }

                3 -> {
                    // ì´ì „ ë³µì¡í•œ í˜•ì‹ì„ ë‹¨ìˆœí•˜ê²Œ ë³€í™˜
                    val fileIndex = parts[0].toIntOrNull() ?: 1
                    val chunkIndex = parts[1].toIntOrNull() ?: 0
                    val lineNumber = chunkIndex * 1000  // ê¸°ì¡´ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¼ì¸ ë²ˆí˜¸ë¡œ ë³€í™˜ (1000ê°œ ë‹¨ìœ„)

                    log.info("[ğŸ”„] ê¸°ì¡´ ë³µì¡í•œ í˜•ì‹ì—ì„œ ë³€í™˜: $fileIndex:$lineNumber")
                    saveOverallProgress(fileIndex, lineNumber)

                    fileIndex to lineNumber
                }

                4 -> {
                    // ì´ì „ 4ê°œ íŒŒë¼ë¯¸í„° í˜•ì‹ì„ ë‹¨ìˆœí•˜ê²Œ ë³€í™˜
                    val fileIndex = parts[0].toIntOrNull() ?: 1
                    val chunkIndex = parts[1].toIntOrNull() ?: 0
                    val lineNumber = chunkIndex * 1000  // ê¸°ì¡´ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¼ì¸ ë²ˆí˜¸ë¡œ ë³€í™˜

                    log.info("[ğŸ”„] ê¸°ì¡´ 4ê°œ íŒŒë¼ë¯¸í„° í˜•ì‹ì—ì„œ ë³€í™˜: $fileIndex:$lineNumber")
                    saveOverallProgress(fileIndex, lineNumber)

                    fileIndex to lineNumber
                }

                else -> {
                    log.warn("[âš ï¸] Progress í˜•ì‹ ì˜¤ë¥˜ ('íŒŒì¼ì¸ë±ìŠ¤:ë¼ì¸ë²ˆí˜¸' í˜•íƒœì—¬ì•¼ í•¨), ì²˜ìŒë¶€í„° ì‹œì‘")
                    1 to 0
                }
            }
        } else {
            log.info("[ğŸ“‹] Progress íŒŒì¼ ì—†ìŒ, ì²˜ìŒë¶€í„° ì‹œì‘")
            1 to 0
        }
    }

    // ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    private fun createCsvMapper(): CsvMapper {
        return CsvMapper().apply {
            disable(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
        }
    }

    private fun skipCsvRows(iterator: Iterator<CulturalBookDto>, skipLines: Int) {
        if (skipLines > 0) {
            log.info("[â©] ${skipLines}í–‰ ìŠ¤í‚µ ì¤‘...")
            var skippedCount = 0
            repeat(skipLines) {
                if (iterator.hasNext()) {
                    try {
                        iterator.next()
                        skippedCount++
                        if (skippedCount % 10000 == 0) {
                            log.info("[â©] ${skippedCount}/${skipLines}í–‰ ìŠ¤í‚µ ì™„ë£Œ...")
                        }
                    } catch (e: Exception) {
                        log.warn("[âš ï¸] ${skippedCount + 1}í–‰ ìŠ¤í‚µ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): ${e.message}")
                        skippedCount++
                    }
                }
            }
            log.info("[âœ…] ìŠ¤í‚µ ì™„ë£Œ. ì´ì œ ${skipLines + 1}í–‰ë¶€í„° ì²˜ë¦¬ ì‹œì‘")
        }
    }

    private fun enrichBookRequest(request: BookCreateRequest): BookCreateRequest {
        val isIsbnSearchable = request.isbn != "UNKNOWN" && request.isbn.isNotBlank()

        val kakaoRequest = KakaoApiRequest(
            query = if (isIsbnSearchable) request.isbn else request.title,
            target = if (isIsbnSearchable) "isbn" else "title",
            size = 1
        )

        val kakaoResponse = kakaoBookSearchClient.searchBook(kakaoRequest)
        val doc = kakaoResponse?.documents?.firstOrNull()
        val publishedDate = doc?.datetime
            ?.takeIf { it.length >= 10 }
            ?.substring(0, 10)
            ?.let { DateUtil.parseFlexibleDate(it) }
            ?: DateUtil.parseFlexibleDate("0001-01-01")

        return if (doc != null) {
            request.copy(
                summary = doc.contents,
                publishedDate = publishedDate,
                detailUrl = doc.url,
                translator = doc.translators,
                price = doc.price,
                titleImage = doc.thumbnail,
                authorNameList = doc.authors,
                publisherName = doc.publisher
            )
        } else {
            log.debug("[âš ï¸] '${request.title} ${request.isbn}' enrich ì‹¤íŒ¨ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
            request
        }
    }

    private fun parseContributors(raw: String?): Pair<List<String>, List<String>> {
        val authors = mutableListOf<String>()
        val translators = mutableListOf<String>()
        raw?.split(",")?.map { it.trim() }?.forEach { person ->
            when {
                person.contains("ì§€ì€ì´") -> authors.add(person.replace("(ì§€ì€ì´)", "").trim())
                person.contains("ì˜®ê¸´ì´") -> translators.add(person.replace("(ì˜®ê¸´ì´)", "").trim())
            }
        }
        return authors to translators
    }
}