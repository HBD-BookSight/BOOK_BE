package com.hbd.book_be.external.loader

import com.fasterxml.jackson.dataformat.csv.CsvMapper
import com.fasterxml.jackson.dataformat.csv.CsvSchema
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule
import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper
import com.hbd.book_be.config.properties.ExternalLoaderProperties
import com.hbd.book_be.dto.request.BookCreateRequest
import com.hbd.book_be.exception.ErrorCodes
import com.hbd.book_be.exception.ValidationException
import com.hbd.book_be.external.kakao.KakaoApiRequest
import com.hbd.book_be.external.kakao.KakaoBookSearchClient
import com.hbd.book_be.external.loader.dto.CulturalBookDto
import com.hbd.book_be.util.DateUtil
import org.slf4j.LoggerFactory
import org.springframework.boot.CommandLineRunner
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty
import org.springframework.jdbc.core.JdbcTemplate
import org.springframework.stereotype.Component
import java.io.BufferedWriter
import java.io.FileWriter
import java.nio.file.Files
import java.nio.file.Paths

@Component
@ConditionalOnProperty(
    prefix = "external.cultural-data-loader",
    name = ["enabled"],
    havingValue = "true",
    matchIfMissing = false
)
class CulturalDatasetLoader(
    jdbcTemplate: JdbcTemplate,
    private val kakaoBookSearchClient: KakaoBookSearchClient,
    private val loaderProperties: ExternalLoaderProperties
) : CommandLineRunner {

    private val log = LoggerFactory.getLogger(CulturalDatasetLoader::class.java)

    private val jdbcRepository = BookJdbcRepository(jdbcTemplate)
    private val mapper = jacksonObjectMapper().apply {
        registerModule(JavaTimeModule())
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
        disable(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)
    }

    private val outputPath = Paths.get(loaderProperties.outputPath)
    private val progressPath = Paths.get(loaderProperties.progressPath ?: "${loaderProperties.outputPath}.progress")
    private val csvChunkSize = 1000
    private val totalFileCount = 32 // dataset-1.csv ~ dataset-32.csv
    private val linesPerFile = 5000 // ê° dataset íŒŒì¼ì˜ ì˜ˆìƒ í–‰ ìˆ˜

    override fun run(vararg args: String?) {
        log.info("[ğŸš€] CulturalDatasetLoader ì‹œì‘ë¨ (external-loader.enabled=true)")

        initializeOutputFiles()
        checkExistingFiles()

        // ì§„í–‰ ìƒí™©ì—ì„œ ì‹œì‘í•  íŒŒì¼ ì¸ë±ìŠ¤ ë¡œë“œ
        val (startFileIndex, startChunkIndex) = loadOverallProgress()

        // 1ë‹¨ê³„: JSONL ìƒì„±
        val allEnrichedRequests = mutableListOf<BookCreateRequest>()

        // ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ì²´í¬
        if (startFileIndex > totalFileCount) {
            log.info("[ğŸ‰] ëª¨ë“  CSV íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. DB ì €ì¥ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
            // JSONLì—ì„œ ë°ì´í„° ë¡œë“œí•´ì„œ DB ì €ì¥
            if (Files.exists(outputPath)) {
                val jsonlData = loadFromJsonl()
                saveToDatabase(jsonlData)
            } else {
                log.warn("[âš ï¸] JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            }
            return
        }

        // dataset-1.csvë¶€í„° dataset-32.csvê¹Œì§€ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for (fileIndex in startFileIndex..totalFileCount) {
            val csvFileName = "dataset-${fileIndex}.csv"
            log.info("[ğŸ“‚] ì²˜ë¦¬ ì¤‘: $csvFileName (${fileIndex}/${totalFileCount})")

            try {
                // ì²« ë²ˆì§¸ ì¬ì‹œì‘ íŒŒì¼ì¸ ê²½ìš° ì €ì¥ëœ ì²­í¬ ì¸ë±ìŠ¤ë¶€í„° ì‹œì‘, ê·¸ ì™¸ì—ëŠ” 0ë¶€í„° ì‹œì‘
                val chunkStartIndex = if (fileIndex == startFileIndex) startChunkIndex else 0
                val enrichedRequests = processCsvFileFromProgress(csvFileName, chunkStartIndex)
                allEnrichedRequests.addAll(enrichedRequests)

                log.info("[âœ…] $csvFileName ì²˜ë¦¬ ì™„ë£Œ (${enrichedRequests.size}ê°œ ë°ì´í„°)")

                // íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ì‹œ ë‹¤ìŒ íŒŒì¼ë¡œ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                saveOverallProgress(fileIndex + 1, 0)

            } catch (e: Exception) {
                log.error("[âŒ] $csvFileName ì²˜ë¦¬ ì‹¤íŒ¨: ${e.message}", e)
                throw e
            }
        }

        // 2ë‹¨ê³„: DB ì €ì¥
        log.info("[ğŸ’¾] JSONL ìƒì„± ì™„ë£Œ. ì´ì œ DB ì €ì¥ ì‹œì‘...")
        saveToDatabase(allEnrichedRequests)

        log.info("[ğŸ‰] ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
    }

    private fun checkExistingFiles() {
        // ê¸°ì¡´ JSONL íŒŒì¼ í™•ì¸
        if (Files.exists(outputPath)) {
            val existingLines = Files.lines(outputPath).use { it.count() }
            log.info("[ğŸ“‹] ê¸°ì¡´ JSONL íŒŒì¼ ë°œê²¬: ${existingLines}ì¤„")
        } else {
            log.info("[ğŸ“‹] ìƒˆë¡œìš´ JSONL íŒŒì¼ ìƒì„± ì˜ˆì •")
        }

        // ê¸°ì¡´ progress.txt í™•ì¸
        if (Files.exists(progressPath)) {
            val progressInfo = Files.readString(progressPath).trim()
            log.info("[ğŸ“‹] ê¸°ì¡´ ì§„í–‰ ìƒí™© ë°œê²¬: $progressInfo")
        } else {
            log.info("[ğŸ“‹] ì²˜ìŒë¶€í„° ì‹œì‘")
        }
    }

    private fun initializeOutputFiles() {
        Files.createDirectories(outputPath.parent)
        Files.createDirectories(progressPath.parent)
    }

    private fun processCsvFileFromProgress(csvFileName: String, startChunkIndex: Int = 0): List<BookCreateRequest> {
        val totalRows = getTotalCsvRows(csvFileName)
        val totalChunks = (totalRows + csvChunkSize - 1) / csvChunkSize
        val skipRows = startChunkIndex * csvChunkSize

        // ì˜ˆìƒ í–‰ ìˆ˜ì™€ ì‹¤ì œ í–‰ ìˆ˜ ë¹„êµ
        if (totalRows != linesPerFile) {
            log.warn("[âš ï¸] $csvFileName: ì˜ˆìƒ ${linesPerFile}í–‰ê³¼ ì‹¤ì œ ${totalRows}í–‰ì´ ë‹¤ë¦…ë‹ˆë‹¤!")
        }

        log.info("[ğŸ“Š] $csvFileName: ì´ ${totalRows}í–‰, ${totalChunks}ì²­í¬ ì¤‘ ${startChunkIndex}ë²ˆì§¸ë¶€í„° ì‹œì‘")
        if (skipRows > 0) {
            log.info("[â­ï¸] ${skipRows}í–‰ ìŠ¤í‚µí•˜ê³  ${skipRows + 1}í–‰ë¶€í„° ì²˜ë¦¬ ì‹œì‘")
        } else {
            log.info("[ğŸ¯] íŒŒì¼ ì‹œì‘ë¶€í„° ì²˜ë¦¬")
        }

        val csvMapper = CsvMapper().apply {
            disable(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
        }
        val schema = CsvSchema.emptySchema().withHeader()
        val allEnrichedRequests = mutableListOf<BookCreateRequest>()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val reader = csvMapper.readerFor(CulturalBookDto::class.java).with(schema)
            val iterator = reader.readValues<CulturalBookDto>(inputStream)

            // ì´ë¯¸ ì²˜ë¦¬ëœ í–‰ë“¤ ìŠ¤í‚µ
            if (skipRows > 0) {
                log.info("[â©] ${skipRows}í–‰ ìŠ¤í‚µ ì¤‘...")
                var skippedCount = 0
                repeat(skipRows) {
                    if (iterator.hasNext()) {
                        try {
                            iterator.next()
                            skippedCount++
                            if (skippedCount % 10000 == 0) {
                                log.info("[â©] ${skippedCount}/${skipRows}í–‰ ìŠ¤í‚µ ì™„ë£Œ...")
                            }
                        } catch (e: Exception) {
                            log.warn("[âš ï¸] ${skippedCount + 1}í–‰ ìŠ¤í‚µ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): ${e.message}")
                            skippedCount++
                        }
                    }
                }
                log.info("[âœ…] ìŠ¤í‚µ ì™„ë£Œ. ì´ì œ ${skipRows + 1}í–‰ë¶€í„° ì²˜ë¦¬ ì‹œì‘")
            }

            var currentChunkIndex = startChunkIndex
            var chunk = mutableListOf<CulturalBookDto>()

            while (iterator.hasNext()) {
                try {
                    val dto = iterator.next()
                    chunk.add(dto)
                } catch (e: Exception) {
                    log.warn("[âš ï¸] CSV í–‰ íŒŒì‹± ì˜¤ë¥˜ (ìŠ¤í‚µ): ${e.message}")
                    continue
                }

                if (chunk.size >= csvChunkSize) {
                    val enrichedRequests = processChunk(chunk, currentChunkIndex)
                    allEnrichedRequests.addAll(enrichedRequests)
                    chunk.clear()
                    currentChunkIndex++

                    // í˜„ì¬ íŒŒì¼ì˜ ì§„í–‰ ìƒí™© ì €ì¥
                    val currentFileIndex = csvFileName.removePrefix("dataset-").removeSuffix(".csv").toInt()
                    saveOverallProgress(currentFileIndex, currentChunkIndex)

                    val processedRows = currentChunkIndex * csvChunkSize
                    val progress = (processedRows.toDouble() / totalRows * 100).toInt()
                    log.info("[ğŸ“Š] $csvFileName: ${processedRows}/${totalRows}í–‰ ì²˜ë¦¬ ì™„ë£Œ (${progress}%) - ì²­í¬ #${currentChunkIndex}")
                }
            }

            // ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if (chunk.isNotEmpty()) {
                val enrichedRequests = processChunk(chunk, currentChunkIndex)
                allEnrichedRequests.addAll(enrichedRequests)
                log.info("[âœ…] $csvFileName: JSONL ìƒì„± ì™„ë£Œ")
            }
        } ?: throw ValidationException(
            "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName",
            ErrorCodes.CSV_FILE_NOT_FOUND
        )

        return allEnrichedRequests
    }

    private fun getTotalCsvRows(csvFileName: String): Int {
        val csvMapper = CsvMapper().apply {
            disable(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
        }
        val schema = CsvSchema.emptySchema().withHeader()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val reader = csvMapper.readerFor(CulturalBookDto::class.java).with(schema)
            val iterator = reader.readValues<CulturalBookDto>(inputStream)

            var count = 0
            while (iterator.hasNext()) {
                try {
                    iterator.next()
                    count++
                } catch (e: Exception) {
                    log.warn("[âš ï¸] $csvFileName í–‰ ${count + 1} íŒŒì‹± ì˜¤ë¥˜ (ì¹´ìš´íŠ¸ë§Œ ì¦ê°€): ${e.message}")
                    count++
                }
            }
            log.info("[ğŸ“Š] $csvFileName ì´ í–‰ ìˆ˜: $count")
            return count
        } ?: throw ValidationException(
            "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName",
            ErrorCodes.CSV_FILE_NOT_FOUND
        )
    }

    private fun processChunk(chunk: List<CulturalBookDto>, chunkIndex: Int): List<BookCreateRequest> {
        val requests = parseToRequests(chunk)
        val enrichedRequests = mutableListOf<BookCreateRequest>()

        requests.forEach { request ->
            val enrichedRequest = enrichBookRequest(request)
            enrichedRequests.add(enrichedRequest)
        }

        // JSONL í˜•íƒœë¡œ ì €ì¥ (DB ì €ì¥ì€ ë‚˜ì¤‘ì—)
        appendRequestsToJsonl(enrichedRequests)

        return enrichedRequests
    }

    private fun appendRequestsToJsonl(requests: List<BookCreateRequest>) {
        BufferedWriter(FileWriter(outputPath.toFile(), true)).use { writer ->
            requests.forEach { request ->
                val jsonString = mapper.writeValueAsString(request)
                writer.write(jsonString)
                writer.write("\n")
            }
        }
    }

    // JSONL ìƒì„± ì™„ë£Œ í›„ DBì— ì €ì¥
    private fun saveToDatabase(allEnrichedRequests: List<BookCreateRequest>) {
        log.info("[ğŸ’¾] ì´ ${allEnrichedRequests.size}ê°œ ë°ì´í„°ë¥¼ DBì— ì €ì¥ ì‹œì‘...")

        allEnrichedRequests.chunked(loaderProperties.batchSize).forEachIndexed { idx, chunk ->
            try {
                jdbcRepository.saveBooksWithJdbc(chunk)
                log.info("[âœ…] ${idx + 1}ë²ˆì§¸ DB ì²­í¬ ì €ì¥ ì„±ê³µ (${chunk.size}ê¶Œ)")
            } catch (e: Exception) {
                log.error("[âŒ] ${idx + 1}ë²ˆì§¸ DB ì²­í¬ ì €ì¥ ì‹¤íŒ¨: ${e.message}", e)
                throw e // ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
            }
        }

        log.info("[ğŸ‰] DB ì €ì¥ ì™„ë£Œ: ${allEnrichedRequests.size}ê°œ ë°ì´í„°")
    }

    // ì „ì²´ ì§„í–‰ ìƒí™© ì €ì¥: "íŒŒì¼ì¸ë±ìŠ¤:ì²­í¬ì¸ë±ìŠ¤" í˜•íƒœ
    private fun saveOverallProgress(fileIndex: Int, chunkIndex: Int) {
        val progressInfo = "$fileIndex:$chunkIndex"
        Files.writeString(progressPath, progressInfo)
    }

    // ì „ì²´ ì§„í–‰ ìƒí™© ë¡œë“œ: (íŒŒì¼ì¸ë±ìŠ¤, ì²­í¬ì¸ë±ìŠ¤) ë°˜í™˜
    private fun loadOverallProgress(): Pair<Int, Int> {
        return if (Files.exists(progressPath)) {
            val progressInfo = Files.readString(progressPath).trim()
            log.info("[ğŸ“‹] Progress íŒŒì¼ ë°œê²¬: '$progressInfo'")

            val parts = progressInfo.split(":")
            if (parts.size == 2) {
                val firstPart = parts[0]
                val secondPart = parts[1].toIntOrNull() ?: 0

                // ê¸°ì¡´ format: "dataset.csv:62" -> ìƒˆë¡œìš´ formatìœ¼ë¡œ ë³€í™˜
                if (firstPart == "dataset.csv") {
                    val totalProcessedChunks = secondPart
                    val totalProcessedRows = totalProcessedChunks * csvChunkSize

                    log.info("[ğŸ”„] ê¸°ì¡´ í˜•ì‹ ê°ì§€: dataset.csv ê¸°ì¤€ ${totalProcessedChunks}ì²­í¬ (${totalProcessedRows}í–‰) ì²˜ë¦¬ë¨")

                    // ë¶„í• ëœ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                    val fileIndex = (totalProcessedRows / linesPerFile) + 1
                    val remainingRows = totalProcessedRows % linesPerFile
                    val chunkIndex = remainingRows / csvChunkSize

                    log.info("[ğŸ”€] ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜: dataset-${fileIndex}.csvì˜ ${chunkIndex}ë²ˆì§¸ ì²­í¬ë¶€í„°")

                    // ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ progress íŒŒì¼ ì—…ë°ì´íŠ¸
                    saveOverallProgress(fileIndex, chunkIndex)

                    return fileIndex to chunkIndex
                }
                // ìƒˆë¡œìš´ format: "íŒŒì¼ì¸ë±ìŠ¤:ì²­í¬ì¸ë±ìŠ¤"
                else {
                    val fileIndex = firstPart.toIntOrNull() ?: 1
                    val chunkIndex = secondPart

                    // ê° íŒŒì¼ë‹¹ ì²­í¬ ìˆ˜ ê³„ì‚° (5000ì¤„ Ã· 1000 = 5ì²­í¬: 0,1,2,3,4)
                    val chunksPerFile = (linesPerFile + csvChunkSize - 1) / csvChunkSize

                    // íŒŒì¼ì´ ì™„ì „íˆ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if (chunkIndex >= chunksPerFile) {
                        // í˜„ì¬ íŒŒì¼ ì™„ë£Œ, ë‹¤ìŒ íŒŒì¼ë¡œ
                        val nextFileIndex = fileIndex + 1
                        log.info("[âœ…] dataset-${fileIndex}.csv ì™„ë£Œë¨. ë‹¤ìŒ íŒŒì¼: dataset-${nextFileIndex}.csvë¡œ ì´ë™")
                        return if (nextFileIndex <= totalFileCount) {
                            nextFileIndex to 0
                        } else {
                            log.info("[ğŸ‰] ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
                            totalFileCount + 1 to 0 // ëª¨ë“  íŒŒì¼ ì™„ë£Œ í‘œì‹œ
                        }
                    }

                    // íŒŒì¼ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ 1ë¶€í„° ì‹œì‘
                    if (fileIndex in 1..totalFileCount) {
                        fileIndex to chunkIndex
                    } else {
                        log.warn("[âš ï¸] íŒŒì¼ ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼ (${fileIndex}), ì²˜ìŒë¶€í„° ì‹œì‘")
                        1 to 0
                    }
                }
            } else {
                log.warn("[âš ï¸] Progress í˜•ì‹ ì˜¤ë¥˜ ('íŒŒì¼ì¸ë±ìŠ¤:ì²­í¬ì¸ë±ìŠ¤' í˜•íƒœì—¬ì•¼ í•¨), ì²˜ìŒë¶€í„° ì‹œì‘")
                1 to 0
            }
        } else {
            log.info("[ğŸ“‹] Progress íŒŒì¼ ì—†ìŒ, ì²˜ìŒë¶€í„° ì‹œì‘")
            1 to 0
        }
    }

    // JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
    private fun loadFromJsonl(): List<BookCreateRequest> {
        val requests = mutableListOf<BookCreateRequest>()
        Files.lines(outputPath).use { lines ->
            lines.forEach { line ->
                if (line.isNotBlank()) {
                    try {
                        val request = mapper.readValue(line, BookCreateRequest::class.java)
                        requests.add(request)
                    } catch (e: Exception) {
                        log.warn("[âš ï¸] JSONL ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: $line")
                    }
                }
            }
        }
        log.info("[ğŸ“–] JSONLì—ì„œ ${requests.size}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return requests
    }

    private fun enrichBookRequest(request: BookCreateRequest): BookCreateRequest {
        val isIsbnSearchable = request.isbn != "UNKNOWN" && request.isbn.isNotBlank()

        val kakaoRequest = KakaoApiRequest(
            query = if (isIsbnSearchable) request.isbn else request.title,
            target = if (isIsbnSearchable) "isbn" else "title",
            size = 1
        )

        val kakaoResponse = kakaoBookSearchClient.searchBook(kakaoRequest)
        val doc = kakaoResponse?.documents?.firstOrNull()
        val publishedDate = doc?.datetime
            ?.takeIf { it.length >= 10 }
            ?.substring(0, 10)
            ?.let { DateUtil.parseFlexibleDate(it) }
            ?: DateUtil.parseFlexibleDate("0001-01-01")

        return if (doc != null) {
            request.copy(
                summary = doc.contents,
                publishedDate = publishedDate,
                detailUrl = doc.url,
                translator = doc.translators,
                price = doc.price,
                titleImage = doc.thumbnail,
                authorNameList = doc.authors,
                publisherName = doc.publisher
            )
        } else {
            log.info("[âš ï¸] '${request.title} ${request.isbn}' enrich ì‹¤íŒ¨ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
            request
        }
    }

    private fun parseToRequests(dataList: List<CulturalBookDto>): List<BookCreateRequest> {
        return dataList.mapNotNull { dto ->
            try {
                val (authors, translators) = parseContributors(dto.authrNm)
                BookCreateRequest(
                    isbn = dto.isbnThirteenNo ?: dto.isbnNo ?: "UNKNOWN",
                    title = dto.titleNm ?: "ì œëª© ì—†ìŒ",
                    summary = dto.bookIntrcnCn.orEmpty(),
                    publishedDate = DateUtil.parseFlexibleDate(
                        (dto.pblicteDe ?: dto.twoPblicteDe).takeIf { !it.isNullOrBlank() } ?: "1001-01-01"
                    ),
                    detailUrl = null,
                    translator = translators,
                    price = dto.prcValue?.toIntOrNull(),
                    titleImage = dto.imageUrl,
                    authorNameList = authors,
                    publisherName = dto.publisherNm ?: "ì•Œ ìˆ˜ ì—†ìŒ"
                )
            } catch (e: Exception) {
                log.info("[âš ï¸] íŒŒì‹± ì‹¤íŒ¨: ${dto.titleNm} (${e.message})")
                null
            }
        }
    }

    private fun parseContributors(raw: String?): Pair<List<String>, List<String>> {
        val authors = mutableListOf<String>()
        val translators = mutableListOf<String>()
        raw?.split(",")?.map { it.trim() }?.forEach { person ->
            when {
                person.contains("ì§€ì€ì´") -> authors.add(person.replace("(ì§€ì€ì´)", "").trim())
                person.contains("ì˜®ê¸´ì´") -> translators.add(person.replace("(ì˜®ê¸´ì´)", "").trim())
            }
        }
        return authors to translators
    }
}