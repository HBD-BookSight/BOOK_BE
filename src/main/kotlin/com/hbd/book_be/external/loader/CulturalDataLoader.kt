package com.hbd.book_be.external.loader

import com.fasterxml.jackson.dataformat.csv.CsvMapper
import com.fasterxml.jackson.dataformat.csv.CsvSchema
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule
import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper
import com.hbd.book_be.config.properties.ExternalLoaderProperties
import com.hbd.book_be.dto.request.BookCreateRequest
import com.hbd.book_be.exception.ErrorCodes
import com.hbd.book_be.exception.ValidationException
import com.hbd.book_be.external.kakao.KakaoApiRequest
import com.hbd.book_be.external.kakao.KakaoBookSearchClient
import com.hbd.book_be.external.loader.dto.CulturalBookDto
import com.hbd.book_be.util.DateUtil
import org.slf4j.LoggerFactory
import org.springframework.boot.CommandLineRunner
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty
import org.springframework.jdbc.core.JdbcTemplate
import org.springframework.stereotype.Component
import java.io.BufferedWriter
import java.io.FileWriter
import java.nio.file.Files
import java.nio.file.Paths

@Component
@ConditionalOnProperty(
    prefix = "external.cultural-data-loader",
    name = ["enabled"],
    havingValue = "true",
    matchIfMissing = false
)
class CulturalDatasetLoader(
    jdbcTemplate: JdbcTemplate,
    private val kakaoBookSearchClient: KakaoBookSearchClient,
    private val loaderProperties: ExternalLoaderProperties
) : CommandLineRunner {

    private val log = LoggerFactory.getLogger(CulturalDatasetLoader::class.java)
    private val jdbcRepository = BookJdbcRepository(jdbcTemplate)
    private val mapper = jacksonObjectMapper().apply {
        registerModule(JavaTimeModule())
        enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
    }

    private val outputPath = Paths.get(loaderProperties.outputPath)
    private val progressPath = Paths.get(loaderProperties.progressPath)
    private val fileSize = 5000
    private val totalFileCount = 32
    private var currentJsonlChunkIndex = 0

    // ====================================
    // 1. ë©”ì¸ ì§„ì…ì  (Main Entry Point)
    // ====================================

    override fun run(vararg args: String?) {
        log.info("[ğŸš€] CulturalDatasetLoader ì‹œì‘")

        initializeDirectories()
        val (startFileIndex, startLineNumber) = loadProgress()
        initializeJsonlChunk()

        if (startFileIndex <= totalFileCount) {
            processAllCsvFiles(startFileIndex, startLineNumber)
        }

        saveJsonlToDatabase()
        log.info("[ğŸ‰] ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
    }

    // ====================================
    // 2. ì´ˆê¸°í™” ê´€ë ¨ (Initialization)
    // ====================================

    private fun initializeDirectories() {
        Files.createDirectories(outputPath)
        Files.createDirectories(progressPath.parent)
    }

    private fun initializeJsonlChunk() {
        if (!Files.exists(outputPath)) {
            currentJsonlChunkIndex = 0
            return
        }

        Files.list(outputPath).use { files ->
            val jsonlFiles = files.filter { it.toString().endsWith(".jsonl") }.sorted().toList()
            if (jsonlFiles.isEmpty()) {
                currentJsonlChunkIndex = 0
                return
            }

            val lastFile = jsonlFiles.last()
            val lineCount = Files.lines(lastFile).use { it.count().toInt() }

            currentJsonlChunkIndex = if (lineCount >= fileSize) {
                jsonlFiles.size
            } else {
                jsonlFiles.size - 1
            }
        }
    }

    // ====================================
    // 3. ì§„í–‰ìƒíƒœ ê´€ë¦¬ (Progress Management)
    // ====================================

    private fun loadProgress(): Pair<Int, Int> {
        if (!Files.exists(progressPath)) return 1 to 0

        val progressInfo = Files.readString(progressPath).trim()
        val parts = progressInfo.split(":")

        return when (parts.size) {
            2 -> {
                val fileIndex = parts[0].toIntOrNull() ?: 1
                val lineNumber = parts[1].toIntOrNull() ?: 0
                fileIndex to lineNumber
            }

            else -> {
                log.warn("[âš ï¸] Progress í˜•ì‹ ì˜¤ë¥˜, ì²˜ìŒë¶€í„° ì‹œì‘")
                1 to 0
            }
        }
    }

    private fun saveProgress(fileIndex: Int, lineNumber: Int) {
        Files.writeString(progressPath, "$fileIndex:$lineNumber")
    }

    // ====================================
    // 4. CSV ì²˜ë¦¬ (CSV Processing)
    // ====================================

    private fun processAllCsvFiles(startFileIndex: Int, startLineNumber: Int) {
        for (fileIndex in startFileIndex..totalFileCount) {
            val csvFileName = "dataset-${fileIndex}.csv"
            log.info("[ğŸ“‚] ì²˜ë¦¬ ì¤‘: $csvFileName (${fileIndex}/${totalFileCount})")

            val startLineIndex = if (fileIndex == startFileIndex) startLineNumber else 0
            processCsvFile(csvFileName, startLineIndex, fileIndex)

            saveProgress(fileIndex + 1, 0)
            log.info("[âœ…] $csvFileName ì²˜ë¦¬ ì™„ë£Œ")
        }
    }

    private fun processCsvFile(csvFileName: String, startLineIndex: Int, fileIndex: Int) {
        val csvMapper = CsvMapper().apply {
            disable(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)
            enable(com.fasterxml.jackson.databind.DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)
        }
        val schema = CsvSchema.emptySchema().withHeader()

        javaClass.getResourceAsStream("/dataset/$csvFileName")?.use { inputStream ->
            val iterator = csvMapper.readerFor(CulturalBookDto::class.java)
                .with(schema)
                .readValues<CulturalBookDto>(inputStream)

            // ìŠ¤í‚µí•  ë¼ì¸ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            repeat(startLineIndex) {
                if (iterator.hasNext()) {
                    try {
                        iterator.next()
                    } catch (e: Exception) { /* ìŠ¤í‚µ ì¤‘ ì˜¤ë¥˜ ë¬´ì‹œ */
                    }
                }
            }

            var currentLineIndex = startLineIndex
            var successCount = 0
            var skipCount = 0

            while (iterator.hasNext()) {
                try {
                    val dto = iterator.next()
                    currentLineIndex++

                    parseToRequest(dto)?.let { request ->
                        enrichBookRequest(request)?.let { enrichedRequest ->
                            appendToJsonl(enrichedRequest)
                            successCount++
                        } ?: skipCount++
                    }

                    if (currentLineIndex % 100 == 0) {
                        saveProgress(fileIndex, currentLineIndex)
                        log.info("[ğŸ“Š] $csvFileName: ${currentLineIndex}í–‰ ì²˜ë¦¬ - ì €ì¥: ${successCount}, ìŠ¤í‚µ: ${skipCount}")
                    }

                } catch (e: Exception) {
                    log.warn("[âš ï¸] CSV í–‰ ${currentLineIndex + 1} ì˜¤ë¥˜: ${e.message}")
                    currentLineIndex++
                }
            }

            log.info("[âœ…] $csvFileName: ì´ ì €ì¥: ${successCount}, ìŠ¤í‚µ: ${skipCount}")

        } ?: throw ValidationException("CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $csvFileName", ErrorCodes.CSV_FILE_NOT_FOUND)
    }

    // ====================================
    // 5. ë°ì´í„° ë³€í™˜ (Data Transformation)
    // ====================================

    private fun parseToRequest(dto: CulturalBookDto): BookCreateRequest? {
        return try {
            val (authors, translators) = parseContributors(dto.authrNm)
            BookCreateRequest(
                isbn = dto.isbnThirteenNo ?: dto.isbnNo ?: "UNKNOWN",
                title = dto.titleNm ?: "ì œëª© ì—†ìŒ",
                summary = dto.bookIntrcnCn.orEmpty(),
                publishedDate = DateUtil.parseFlexibleDate(
                    (dto.pblicteDe ?: dto.twoPblicteDe).takeIf { !it.isNullOrBlank() } ?: "1001-01-01"
                ),
                detailUrl = null,
                translator = translators,
                price = dto.prcValue?.toIntOrNull(),
                titleImage = dto.imageUrl,
                authorNameList = authors,
                publisherName = dto.publisherNm ?: "ì•Œ ìˆ˜ ì—†ìŒ"
            )
        } catch (e: Exception) {
            log.debug("[âš ï¸] íŒŒì‹± ì‹¤íŒ¨: ${dto.titleNm} (${e.message})")
            null
        }
    }

    private fun parseContributors(raw: String?): Pair<List<String>, List<String>> {
        val authors = mutableListOf<String>()
        val translators = mutableListOf<String>()

        raw?.split(",")?.forEach { person ->
            val trimmed = person.trim()
            when {
                trimmed.contains("ì§€ì€ì´") -> authors.add(trimmed.replace("(ì§€ì€ì´)", "").trim())
                trimmed.contains("ì˜®ê¸´ì´") -> translators.add(trimmed.replace("(ì˜®ê¸´ì´)", "").trim())
            }
        }

        return authors to translators
    }

    // ====================================
    // 6. ì™¸ë¶€ API ì—°ë™ (External API Integration)
    // ====================================

    private fun enrichBookRequest(request: BookCreateRequest): BookCreateRequest? {
        val isIsbnSearchable = request.isbn != "UNKNOWN" && request.isbn.isNotBlank()

        val kakaoRequest = KakaoApiRequest(
            query = if (isIsbnSearchable) request.isbn else request.title,
            target = if (isIsbnSearchable) "isbn" else "title",
            size = 1
        )

        val doc = kakaoBookSearchClient.searchBook(kakaoRequest)?.documents?.firstOrNull()
            ?: return null

        val publishedDate = doc.datetime
            ?.takeIf { it.length >= 10 }
            ?.substring(0, 10)
            ?.let { DateUtil.parseFlexibleDate(it) }
            ?: DateUtil.parseFlexibleDate("0001-01-01")

        return request.copy(
            summary = doc.contents,
            publishedDate = publishedDate,
            detailUrl = doc.url,
            translator = doc.translators,
            price = doc.price,
            titleImage = doc.thumbnail,
            authorNameList = doc.authors,
            publisherName = doc.publisher
        )
    }

    // ====================================
    // 7. JSONL íŒŒì¼ ê´€ë¦¬ (JSONL File Management)
    // ====================================

    private fun appendToJsonl(request: BookCreateRequest) {
        val currentFile = outputPath.resolve("chunk-${String.format("%03d", currentJsonlChunkIndex)}.jsonl")

        // í˜„ì¬ íŒŒì¼ì´ ê°€ë“ ì°¼ëŠ”ì§€ í™•ì¸
        val currentLines = if (Files.exists(currentFile)) {
            Files.lines(currentFile).use { it.count().toInt() }
        } else 0

        if (currentLines >= fileSize) {
            currentJsonlChunkIndex++
        }

        val targetFile = outputPath.resolve("chunk-${String.format("%03d", currentJsonlChunkIndex)}.jsonl")
        BufferedWriter(FileWriter(targetFile.toFile(), true)).use { writer ->
            writer.write(mapper.writeValueAsString(request))
            writer.newLine()
        }
    }

    // ====================================
    // 8. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (Database Persistence)
    // ====================================

    private fun saveJsonlToDatabase() {
        if (!Files.exists(outputPath)) {
            log.warn("[âš ï¸] JSONL ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        }

        Files.list(outputPath).use { files ->
            val jsonlFiles = files.filter { it.toString().endsWith(".jsonl") }.sorted().toList()

            log.info("[ğŸ“–] ì´ ${jsonlFiles.size}ê°œ JSONL íŒŒì¼ì—ì„œ ìŠ¤íŠ¸ë¦¬ë° DB ì €ì¥ ì‹œì‘")
            var totalSaved = 0
            var batch = mutableListOf<BookCreateRequest>()
            var batchIndex = 1

            jsonlFiles.forEach { jsonlFile ->
                Files.lines(jsonlFile).use { lines ->
                    lines.forEach { line ->
                        if (line.isNotBlank()) {
                            try {
                                val request = mapper.readValue(line, BookCreateRequest::class.java)
                                batch.add(request)

                                if (batch.size >= loaderProperties.batchSize) {
                                    jdbcRepository.saveBooksWithJdbc(batch)
                                    totalSaved += batch.size
                                    log.info("[âœ…] ${batchIndex}ë²ˆì§¸ ë°°ì¹˜ ì €ì¥: ${batch.size}ê¶Œ")
                                    batch.clear()
                                    batchIndex++
                                }
                            } catch (e: Exception) {
                                log.warn("[âš ï¸] JSONL ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: $line")
                            }
                        }
                    }
                }
            }

            // ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥
            if (batch.isNotEmpty()) {
                jdbcRepository.saveBooksWithJdbc(batch)
                totalSaved += batch.size
            }

            log.info("[ğŸ‰] DB ì €ì¥ ì™„ë£Œ: ì´ ${totalSaved}ê°œ")
        }
    }
}